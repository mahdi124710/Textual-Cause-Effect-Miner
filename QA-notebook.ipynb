{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finding Causal Relations With Question Answering Models\n",
        "In this Jupyter notebook, we are going to train three distinct models, each with a specific role:\n",
        "\n",
        "1. **Causal Marker Model**: This model's task is to identify the causal marker within a given sentence.\n",
        "2. **Cause Identification Model**: Once we have the causal marker and the sentence, this model is designed to pinpoint the cause.\n",
        "3. **Effect Identification Model**: With the causal marker and the sentence at hand, this model's job is to determine the effect.\n",
        "\n",
        "Before we proceed with training these models, we need to ensure that all necessary dependencies are installed."
      ],
      "metadata": {
        "id": "GfEijTvFgT3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DFGjhXKcHb-Y"
      },
      "outputs": [],
      "source": [
        "!pip install datasets | grep -v 'already satisfied'\n",
        "!pip install transformers | grep -v 'already satisfied'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "from datasets import Dataset\n",
        "import datasets\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from pathlib import Path\n",
        "from tools import run_model"
      ],
      "metadata": {
        "id": "22yubsB7HhUJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data & Model\n",
        "In this section, we perform two main tasks:\n",
        "\n",
        "1. **Data Preparation**: We read the data from a JSON file and transform it into a suitable dataset format for our models.\n",
        "2. **Model Initialization**: We set up the initial configurations for our models.\n",
        "\n",
        "You have the flexibility to train any of the three models mentioned earlier. To do so, simply select the appropriate file corresponding to the model you wish to train."
      ],
      "metadata": {
        "id": "-gI9k8p4jH4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "tokenizer, config = AutoTokenizer.from_pretrained(model_name), AutoConfig.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "file = json.load(open('data_effect.json', 'r', encoding='utf-8'))\n",
        "df = pd.json_normalize(file['data']).sample(frac=1, random_state=10) # 3080\n",
        "dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())\n",
        "train_data = Dataset(pa.Table.from_pandas(df.iloc[:2400]))\n",
        "validation_data = Dataset(pa.Table.from_pandas(df.iloc[2400: ]))\n",
        "data = datasets.DatasetDict({\"train\":train_data,\"validation\": validation_data})\n"
      ],
      "metadata": {
        "id": "GVeIGpL4HkFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess\n",
        "This function is designed to transform our dataset into a format that is compatible with Question-Answering (QA) models."
      ],
      "metadata": {
        "id": "JBuZDmDCjbJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(examples):\n",
        "    \"\"\"\n",
        "    Prepare the data to be fed into QA model.\n",
        "\n",
        "    :param examples: A dataset containing context and answer and question\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_examples = tokenizer(examples[\"question\"], examples[\"context\"], return_offsets_mapping=True)\n",
        "    tokenized_examples['start_positions'], tokenized_examples['end_positions'] = [], []\n",
        "\n",
        "    cls_index = 0\n",
        "    for i, offset in enumerate(tokenized_examples['offset_mapping']):\n",
        "        answer = examples['answers'][i][0]\n",
        "\n",
        "        types = np.array(tokenized_examples.sequence_ids(i))\n",
        "        types[types == None] = 0\n",
        "        types.astype(int)\n",
        "\n",
        "        if len(answer['text'][0]) == 0:\n",
        "            s, e = cls_index, cls_index\n",
        "\n",
        "        else:\n",
        "            s_diff = np.abs(np.array([offset[idx][0] - answer['answer_start'][0] for idx in range(len(offset))]))\n",
        "            s = np.argmin([s_diff[idx] + 100 * (1 - types[idx]) for idx in range(len(s_diff))])\n",
        "\n",
        "            e_diff = np.abs(np.array(\n",
        "                [offset[idx][1] - answer['answer_start'][0] - len(answer['text'][0]) for idx in range(len(offset))]))\n",
        "            e = np.argmin([e_diff[idx] + 100 * (1 - types[idx]) for idx in range(len(e_diff))])\n",
        "\n",
        "        tokenized_examples['start_positions'].append(s)\n",
        "        tokenized_examples['end_positions'].append(e)\n",
        "\n",
        "    tokenized_examples.pop('offset_mapping')\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "Xrg1v6_Hi7my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "Once the data is ready, we feed it into our model to begin the training process."
      ],
      "metadata": {
        "id": "va9E8ATijq55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = data.map(preprocess, batched=True, remove_columns=data[\"train\"].column_names)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"result\",\n",
        "    evaluation_strategy = \"steps\", # 'epochs'\n",
        "    eval_steps = 12,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_ds['train'],\n",
        "    eval_dataset=tokenized_ds['validation'],\n",
        "    tokenizer=tokenizer)\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "ijbtXokwH2gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive\n",
        "This section provides you with the capability to interact with Google Drive. You can utilize this feature to:\n",
        "1. **Save your Trained Models**: After training, you can store your models directly to Google Drive for future use.\n",
        "2. **Load Pre-Trained Models**: If you have models that were previously trained and saved in Google Drive, you can easily load them from here for use in your current project."
      ],
      "metadata": {
        "id": "I4fDHK2kdmzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#trainer.save_model('/content/gdrive/My Drive/effect')"
      ],
      "metadata": {
        "id": "wsid553qH5Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "In this section, we will be working with a test dataset consisting of 300 sentences. Here's the process:\n",
        "\n",
        "1. **Load Pre-Trained Models**: We start by loading our pre-trained models.\n",
        "2. **Test the First Model**: We use the first model to identify the causal marker within each sentence in our test data.\n",
        "3. **Test the Second and Third Models**: Once we have the causal marker, we input both the sentence and the marker into our second and third models. These models will then identify the cause and effect respectively.\n",
        "4. **Display Results**: Finally, we present the results for each test sample.\n",
        "\n",
        "This way, you can evaluate the performance of our models on unseen data."
      ],
      "metadata": {
        "id": "2y6dVpBCdxNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdp = '/content/gdrive/My Drive/'\n",
        "paths = [gdp + 'marker', gdp + 'cause', gdp + 'effect']\n",
        "tokenizers = [AutoTokenizer.from_pretrained(paths[i]) for i in range(3)]\n",
        "models = [AutoModelForQuestionAnswering.from_pretrained(paths[i]) for i in range(3)]\n",
        "\n",
        "lines = open('test.txt', mode='r', encoding='utf-8').readlines()\n",
        "texts = [s.replace('*', '').replace('+', '').replace('&', '') for s in lines]\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "  mark = run_model(models[0], tokenizers[0], text, 'به دلیل این که - نتیجه - علت - زیرا - استنتاج - درصورتی که')\n",
        "  caus = run_model(models[1], tokenizers[1], text, mark)\n",
        "  effe = run_model(models[2], tokenizers[2], text, mark)\n",
        "  answer = [mark, caus, effe] if mark != '[CLS]' else ['', '', '']\n",
        "\n",
        "  print(i)\n",
        "  print(lines[i], end='')\n",
        "  parts = ['marker: ', 'cause: ', 'effect: ']\n",
        "\n",
        "  for j, tchar in enumerate(['&', '*', '+']):\n",
        "    print(parts[j], end='')\n",
        "    print(answer[j], end='    ')\n",
        "\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "61TiSby1E1zF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}